---
title: "P8105: Data Science I"
author: "Assignment 5<br>Jimmy Kelliher (UNI: jmk2303)"
output:
  github_document:
    toc: TRUE
---

<!------------------------------------------------------------------------------
Preamble
------------------------------------------------------------------------------->

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# load necessary packages
library(tidyverse)

# set knitr defaults
knitr::opts_chunk$set(
    echo      = TRUE
  , message   = FALSE
  , fig.width = 6
  , fig.asp   = .6
  , out.width = "90%"
)

# set theme defaults
theme_set(
  theme_bw() +
  theme(
    legend.position = "bottom"
    , plot.title    = element_text(hjust = 0.5)
    , plot.subtitle = element_text(hjust = 0.5)    
    , plot.caption  = element_text(hjust = 0.0)
  )
)

# set color scale defaults
options(
    ggplot2.continuous.colour = "viridis"
  , ggplot2.continuous.fill   = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete   = scale_fill_viridis_d
```

<!------------------------------------------------------------------------------
Problem 1
------------------------------------------------------------------------------->

# Problem 1

We begin by pulling and tidying data on homicides in 50 of the largest US cities, courtesy of The Washington Post.

```{r}
# identify the proper url for the data
path <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

# pull and tidy data
homicides <-
  # read in data and rename a few anomalous missing values
  read_csv(path, na = c("", "Unknown")) %>%
  # create variables useful for analysis
  mutate(
    # concatenate city and state into a single string variable 
      city_state = paste0(city, ", ", state)
    # create a binary factor variable that indicates if a case was solved
    , resolution = factor(case_when(
        disposition == "Closed by arrest"      ~ "Solved"
      , disposition == "Closed without arrest" ~ "Unsolved"
      , disposition == "Open/No arrest"        ~ "Unsolved"
    ))
  ) %>%
  # remove a row that seems to have an error
  filter(city_state != "Tulsa, AL") %>%
  # relocate city_state to be the first column of our data frame
  relocate(city_state)

# output raw data as table
head(homicides, 10) %>% knitr::kable()
```

Upon filtering out a single row with a data entry issue, the dataset consists of `r nrow(homicides)` observations and `r ncol(homicides)` variables, two of which - `city_state` and `resolution` - we added for convenience. Each row of the dataset corresponds to a homicide case reported between `r homicides %>% pull(reported_date) %>% substr(1, 4) %>% as.numeric() %>% unique() %>% min()` and `r homicides %>% pull(reported_date) %>% substr(1, 4) %>% as.numeric() %>% unique() %>% max()` and includes record of the victim's name, race/ethnicity, age, and sex. We also have data on the location of the crime down to the latitude and longitude, as well as whether the crime was ever solved. Across all observations, about `r round(100 * (homicides %>% filter(resolution == "Unsolved") %>% nrow()) / nrow(homicides), 1)`\% of all homicides were never solved. We aim to understand the distribution of resolution rates across cities. To that end, we provide a table of the number of unsolved homicides and the total number of hommicides for each city.

```{r}
# create table of counts and rates of unsolved homicides by city
homicides %>%
  # group by city
  group_by(city_state) %>%
  # summarize data to obtain counts of unsolved homicides and total homicides
  summarize(
      unsolved = sum(resolution == "Unsolved")
    , total    = n()
  ) %>%
  # compute rate of unsolved homicide
  mutate(rate = 100 * unsolved / total) %>%
  # output as readable table
  knitr::kable(
      caption     = "**Count and Rate of Unsolved Homicides, by City**"
    , col.names   = c("City", "Unsolved", "Total", "Rate (in %)")
    , format.args = list(big.mark = ",")
    , d           = 1
  )
```

Let's first consider Baltimore, MD in isolation before we generalize our code to the broader dataset. We want to construct a point estimate and confidence interval for the true proportion of homicides that went unsolved.

```{r}
# create a summary row
baltimoreSummary <-
  # select data
  homicides %>%
  # restrict to Baltimore
  filter(city_state == "Baltimore, MD") %>%
  # summarize data to obtain counts of unsolved homicides and total homicides
  summarize(
      unsolved = sum(resolution == "Unsolved")
    , total    = n()
  )

# apply summary data to test of proportions and extract relevant output
baltimoreTest <-
  # conduct test of proportions
  prop.test(
    x = pull(baltimoreSummary, unsolved)
  , n = pull(baltimoreSummary, total)
  ) %>%
  # tidy test output
  broom::tidy() %>%
  # extract relevant output
  select(estimate, conf.low, conf.high)

# output findings as table
baltimoreTest %>% knitr::kable()
```

We find that in Baltimore between the years 2007 and 2017, the proportion of homicides that went unsolved was about `r round(100 * pull(baltimoreTest, estimate), 1)`\%, and the 95% confidence interval around this point estimate was about [`r round(100 * pull(baltimoreTest, conf.low), 1)`\%, `r round(100 * pull(baltimoreTest, conf.high), 1)`\%]. As we have obtained the desired output, we are now prepared to construct a function that can generalize this procedure.

```{r}
# construct a function to obtain point estimates and confidence intervals
propTestFun <- function(cityData) {

  # create a summary row
  citySummary <-
    # select data
    cityData %>%
    # summarize data to obtain counts of unsolved homicides and total homicides
    summarize(
        unsolved = sum(resolution == "Unsolved")
      , total    = n()
    )

  # apply summary data to test of proportions and extract relevant output
  cityTest <-
    # conduct test of proportions
    prop.test(
      x = pull(citySummary, unsolved)
    , n = pull(citySummary, total)
    ) %>%
    # tidy test output
    broom::tidy() %>%
    # extract relevant output
    select(estimate, conf.low, conf.high)

  # output findings
  cityTest

}

# apply function to nested dataset and un-nest results
cityHomicidesResults <-
  # select original dataset
  homicides %>%
  # nest dataset at the city level
  nest(data = uid:resolution) %>%
  # apply function to each nested city data frame
  mutate(results = map(data, propTestFun)) %>%
  # select relevant features
  select(city_state, results) %>%
  # un-nest results
  unnest(cols = results)

# output results as table
head(cityHomicidesResults, 10) %>% knitr::kable()
```

Using this data frame of results, we can now better visualize the distribution of proportions, sorted according to the point estimate of the proportion of unsolved homicides.

```{r, fig.asp = 1.2}
# create error bar chart of rate of unsolved homicide by city
cityHomicidesResults %>%
  # reorder cities according to point estimate of proportion
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  # instantiate plot
  ggplot(aes(x = city_state, y = estimate)) +
  # add point estimates
  geom_point() +
  # add confidence intervals
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  # flip axes for readability
  coord_flip() +
  # add meta-data
  labs(
    title = "Proportion of Unsolved Homicides, by City"
    , x = "Proportion"
    , y = ""
    , caption  = paste0(
          "Note: Confidence intervals computed at the 95% cinfidence level."
        , "\nSource: The Washington Post."
    )
  )
```

Richmond, VA has the lowest rate of unsolved homicides among these 50 cities. Chicago, IL sits at the other extreme of this distribution. In fact, as its confidence interval does not overlap with that of any other city, it would appear to be an outlier in this sense.

<!------------------------------------------------------------------------------
Problem 2
------------------------------------------------------------------------------->

# Problem 2

```{r}
# create tidy dataset for longitudinal study
longData <-
  # create data frame with file names
  tibble(files = list.files("datasets/longitudinal_study")) %>%
  # add a column where each row contains the contents of the corresponding file
  mutate(raw_data = map(
      .x = files
    , ~read_csv(paste0("datasets/longitudinal_study/", .x))
  )) %>%
  # unnest the nested data frames to create an un-tidied, but usable data frame
  unnest(cols = raw_data) %>%
  # pivot longer via week number
  pivot_longer(
      cols         = starts_with("week_")
    , names_to     = "week"
    , names_prefix = "week_"
  ) %>%
  # separate file name into arm type and subject id; use sensible classes
  mutate(
      arm        = factor(recode(
        substr(files, 1, 3)
      , con = "Control"
      , exp = "Experimental"
    ))
    , subject_id = as.integer(substr(files, 5, 6))
    , week       = as.integer(week)
  ) %>%
  # remove file name column now that it is redundant
  select(  arm, subject_id, week, value) %>%
  # relocate columns by group hierarchy
  relocate(arm, subject_id, week, value)

# output tidied dataset
head(longData, 10) %>% knitr::kable()
```

```{r}
# create a spaghetti chart of the data
longData %>%
  # instantiate plot
  ggplot(aes(x = week, y = value, group = subject_id)) +
  # add lines
  geom_line() +
  # create separate line charts for each arm
  facet_grid(~arm) +
  # add meta-data
  labs(
      title = "Value over Time, by Arm"
    , x     = "Week"
    , y     = "Value"
  )
```

<!------------------------------------------------------------------------------
Problem 3
------------------------------------------------------------------------------->

# Problem 3

```{r}
# set seed
set.seed(10)

# edit dataset to introduce missing values
irisMissing <-
  # call dataset
  iris %>% 
  # randomly select 20 entries from each column and assign them missing values
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  # convert Species to a character vector
  mutate(Species = as.character(Species))

# write a function to address missing values
replaceMissing <- function(v) {
  # if input vector v is a numeric vector...
  if (is.numeric(v)) {
    # ... replace missing values with the mean of non-missing values
    replace_na(v, mean(v, na.rm = TRUE))
  # if input vector v is a character vector...
  } else if (is.character(v)) {
    # ... replace missing values with the string "virginica"
    replace_na(v, "virginica")
  }
}

# apply function to each column of edited dataset to fill in missing values
irisCorrected <- map_df(irisMissing, replaceMissing)

# identify missing rows in edited dataset
missingRows <- sort(unique(unlist(map(irisMissing, ~which(is.na(.x))))))

# output a few rows of interest in edited dataset
head(  irisMissing[missingRows, ], 10) %>% knitr::kable()

# output a few rows of interest in corrected dataset
head(irisCorrected[missingRows, ], 10) %>% knitr::kable()
```
